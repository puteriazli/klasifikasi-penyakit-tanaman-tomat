{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persiapan Librari dan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import mahotas as mt\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Semua librari berhasil diimpor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\dataset\\data sekunder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "print(\"Kategori ditemukan:\", categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dan Segmentasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_segment(image_path):\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Gagal membaca gambar dari path: {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image, (512, 512))\n",
    "    brightness = 5\n",
    "    adjusted_image = cv2.convertScaleAbs(resized_image, alpha=1, beta=brightness)\n",
    "    blurred_image = cv2.GaussianBlur(adjusted_image, (7, 7), 0)\n",
    "    hsv = cv2.cvtColor(blurred_image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    lower_green = np.array([35, 30, 30])\n",
    "    upper_green = np.array([90, 255, 255])\n",
    "    lower_brown = np.array([10, 30, 10])\n",
    "    upper_brown = np.array([80, 255, 255])\n",
    "    lower_yellow = np.array([10, 50, 40])\n",
    "    upper_yellow = np.array([40, 255, 255])\n",
    "\n",
    "    mask_green = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    mask_brown = cv2.inRange(hsv, lower_brown, upper_brown)\n",
    "    mask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "    mask_combined = cv2.bitwise_or(mask_green, cv2.bitwise_or(mask_brown, mask_yellow))\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask_cleaned = cv2.morphologyEx(mask_combined, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask_cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cleaned_mask = np.zeros_like(mask_cleaned)\n",
    "    min_contour_area = 2500\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = w / h\n",
    "        if area > min_contour_area and 0.1 < aspect_ratio < 3.0:\n",
    "            cv2.drawContours(cleaned_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    segmented = cv2.bitwise_and(resized_image, resized_image, mask=cleaned_mask)\n",
    "\n",
    "    return segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ekstraksi Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians\n",
    "bins = 8\n",
    "\n",
    "def extract_hsv_histogram(segmented):\n",
    "    hsv = cv2.cvtColor(segmented, cv2.COLOR_RGB2HSV)\n",
    "    hist_h = cv2.calcHist([hsv], [0], None, [bins], [0, 180])\n",
    "    hist_s = cv2.calcHist([hsv], [1], None, [bins], [0, 256])\n",
    "    hist_v = cv2.calcHist([hsv], [2], None, [bins], [0, 256])\n",
    "    hist_features = np.concatenate([hist_h.flatten(), hist_s.flatten(), hist_v.flatten()])\n",
    "    return hist_features / np.sum(hist_features)\n",
    "\n",
    "def extract_glcm_features(segmented):\n",
    "    gray = cv2.cvtColor(segmented, cv2.COLOR_RGB2GRAY)\n",
    "    angles = [radians(0), radians(45), radians(90), radians(135)]\n",
    "    glcm = graycomatrix(gray, distances=[1], angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    contrast = np.mean(graycoprops(glcm, 'contrast'))\n",
    "    correlation = np.mean(graycoprops(glcm, 'correlation'))\n",
    "    energy = np.mean(graycoprops(glcm, 'energy'))\n",
    "    homogeneity = np.mean(graycoprops(glcm, 'homogeneity'))\n",
    "\n",
    "    return np.array([contrast, correlation, energy, homogeneity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "\n",
    "    for image_name in os.listdir(category_path):\n",
    "        image_path = os.path.join(category_path, image_name)\n",
    "        \n",
    "        segmented = preprocess_segment(image_path)\n",
    "        if segmented is not None:\n",
    "            hsv_features = extract_hsv_histogram(segmented)\n",
    "            glcm_features = extract_glcm_features(segmented)\n",
    "\n",
    "            features = np.concatenate((hsv_features, glcm_features))\n",
    "            X.append(features)\n",
    "            y.append(category)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Dataset siap dengan {X.shape[0]} sampel dan {X.shape[1]} fitur.\")\n",
    "\n",
    "np.savez(\"static/feature extraction/dataset_tomat_features.npz\", X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melihat Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"static/Feature Extraction/dataset_tomat_features.npz\", allow_pickle=True)\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "print(f\"Dataset dimuat dengan {X.shape[0]} sampel dan {X.shape[1]} fitur.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan semua label unik\n",
    "unique_labels = np.unique(y)\n",
    "print(\"Label unik dalam dataset:\", unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pelatihan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [ 'scale', 'auto'],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "grid_search = GridSearchCV(svm, param_grid,cv=5,scoring='accuracy', n_jobs=-1, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Akurasi Model SVM: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision function shape:\", best_svm.decision_function_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpan dan Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_svm, \"model/svm_model.pkl\")\n",
    "joblib.dump(scaler, \"model/scaler.pkl\")\n",
    "\n",
    "print(\"\\nModel SVM berhasil disimpan sebagai 'svm_model.pkl' dan 'scaler.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = joblib.load(\"model/svm_model.pkl\")\n",
    "scaler = joblib.load(\"model/scaler.pkl\")\n",
    "\n",
    "def predict_tomato_disease(features):\n",
    "    features = np.array(features).reshape(1, -1)\n",
    "    features = scaler.transform(features)\n",
    "    prediction = svm_model.predict(features)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi dan Visualisasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasi Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"cividis\", xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, class_label in enumerate(categories):\n",
    "    TP = conf_matrix[i, i]\n",
    "    FN = conf_matrix[i, :].sum() - TP\n",
    "    FP = conf_matrix[:, i].sum() - TP\n",
    "    TN = conf_matrix.sum() - (TP + FP + FN)\n",
    "\n",
    "    print(f\"Class {class_label}: TP={TP}, FN={FN}, FP={FP}, TN={TN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akurasi Data Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = accuracy_score(y_train, svm_model.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Akurasi Training: {train_accuracy:.4f}\")\n",
    "print(f\"Akurasi Testing: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = accuracy_score(y_train, svm_model.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "labels = ['Training', 'Testing']\n",
    "accuracies = [train_accuracy, test_accuracy]\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(5, 5))\n",
    "palette = sns.color_palette(\"pastel\")\n",
    "ax = sns.barplot(x=labels, y=accuracies, palette=palette)\n",
    "\n",
    "for i, acc in enumerate(accuracies):\n",
    "    ax.text(i, acc + 0.02, f'{acc:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('Perbandingan Akurasi Model SVM', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Akurasi', fontsize=12)\n",
    "plt.ylim(0, 1.05)\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report Data 80:20\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Akurasi per fold:\", scores)              # semua skor fold\n",
    "print(\"Rata-rata akurasi:\", scores.mean())      # rata-rata\n",
    "print(\"Standar deviasi:\", scores.std())         # deviasi standar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediksi Gambar Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_primer = r\"D:\\dataset\\data primer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_prediksi = []\n",
    "\n",
    "for image_name in os.listdir(folder_primer):\n",
    "    image_path = os.path.join(folder_primer, image_name)\n",
    "    \n",
    "    if not image_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        segmented = preprocess_segment(image_path)\n",
    "        if segmented is not None:\n",
    "            hsv_features = extract_hsv_histogram(segmented)\n",
    "            glcm_features = extract_glcm_features(segmented)\n",
    "            features = np.concatenate((hsv_features, glcm_features))\n",
    "            predicted_label = predict_tomato_disease(features)\n",
    "\n",
    "            hasil_prediksi.append([image_name, predicted_label])\n",
    "    except Exception as e:\n",
    "        hasil_prediksi.append([image_name, f\"Error: {e}\"])\n",
    "\n",
    "# Simpan ke CSV\n",
    "df_prediksi = pd.DataFrame(hasil_prediksi, columns=[\"Nama Gambar\", \"Prediksi\"])\n",
    "df_prediksi.to_csv(\"static/primer prediction/hasil_prediksi_data_primer.csv\", index=False)\n",
    "print(\"\\nHasil prediksi disimpan ke 'hasil_prediksi_data_primer.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpan File CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((X, y))\n",
    "hsv_columns = [f'H_bin_{i}' for i in range(bins)] + \\\n",
    "              [f'S_bin_{i}' for i in range(bins)] + \\\n",
    "              [f'V_bin_{i}' for i in range(bins)]\n",
    "glcm_columns = ['contrast', 'correlation', 'energy', 'homogeneity']\n",
    "columns = hsv_columns + glcm_columns + ['label']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv(\"static/feature extraction/fitur-daun.csv\", index=False)\n",
    "\n",
    "print(\"Dataset berhasil disimpan dalam format CSV dengan\", len(columns), \"kolom.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Dataset Hasil Ekstraksi Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('static/feature extraction/fitur-daun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_busuk_daun = data[data['label'] == 'busuk daun'].head(2)\n",
    "filtered_data_jamur_daun = data[data['label'] == 'jamur daun'].head(2)\n",
    "filtered_data_sehat = data[data['label'] == 'sehat'].head(2)\n",
    "filtered_data_septoria = data[data['label'] == 'septoria'].head(2)\n",
    "filtered_data = pd.concat([filtered_data_busuk_daun, filtered_data_jamur_daun, filtered_data_sehat, filtered_data_septoria])\n",
    "pd.set_option('display.max_columns', 29)\n",
    "filtered_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
